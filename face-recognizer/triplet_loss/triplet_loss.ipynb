{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466bdda9-563b-45cc-a50c-6708e39e8394",
   "metadata": {},
   "source": [
    "# 1.Crop dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749ea50-1db5-4ecf-a7cd-45edc0142f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "# Define function to load images, detect faces, and crop them with margin\n",
    "def detect_and_crop_faces(base_path, output_path, margin=0.2):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    for person_name in os.listdir(base_path):\n",
    "        person_path = os.path.join(base_path, person_name)\n",
    "        if os.path.isdir(person_path):\n",
    "            # Create output directory for each person\n",
    "            person_output_path = os.path.join(output_path, person_name)\n",
    "            os.makedirs(person_output_path, exist_ok=True)\n",
    "\n",
    "            for image_name in os.listdir(person_path):\n",
    "                image_path = os.path.join(person_path, image_name)\n",
    "                try:\n",
    "                    # Read image\n",
    "                    image = cv2.imread(image_path)\n",
    "                    if image is None:\n",
    "                        print(f\"Skipping {image_path}, not a valid image.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert the image to RGB as MediaPipe expects RGB input\n",
    "                    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # Detect faces\n",
    "                    results = face_detection.process(image_rgb)\n",
    "\n",
    "                    if results.detections:\n",
    "                        for i, detection in enumerate(results.detections):\n",
    "                            # Get bounding box coordinates\n",
    "                            bboxC = detection.location_data.relative_bounding_box\n",
    "                            h, w, _ = image.shape\n",
    "                            \n",
    "                            # Calculate margins\n",
    "                            x_min = int((bboxC.xmin - margin) * w)\n",
    "                            y_min = int((bboxC.ymin - margin) * h)\n",
    "                            x_max = int((bboxC.xmin + bboxC.width + margin) * w)\n",
    "                            y_max = int((bboxC.ymin + bboxC.height + margin) * h)\n",
    "\n",
    "                            # Ensure the coordinates are within image bounds\n",
    "                            x_min = max(0, x_min)\n",
    "                            y_min = max(0, y_min)\n",
    "                            x_max = min(w, x_max)\n",
    "                            y_max = min(h, y_max)\n",
    "\n",
    "                            # Crop the face with margin\n",
    "                            cropped_face = image[y_min:y_max, x_min:x_max]\n",
    "                            face_gray = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                            # Save the cropped face\n",
    "                            output_image_path = os.path.join(\n",
    "                                person_output_path, f\"{os.path.splitext(image_name)[0]}_face_{i}.jpg\"\n",
    "                            )\n",
    "                            cv2.imwrite(output_image_path, face_gray)\n",
    "                            print(f\"Saved cropped face to {output_image_path}\")\n",
    "                    else:\n",
    "                        print(f\"No face detected in {image_path}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "base_path = \"dataset\"\n",
    "output_path = \"new_grayscale\"\n",
    "detect_and_crop_faces(base_path, output_path, margin=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f84cdc-e2ca-42ca-9c96-6e42ce8898af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop one image\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "output_dir = \"testing_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "image_path = \"20241122_173529.jpg\" \n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "if image is None:\n",
    "    print(f\"Error: Image file '{image_path}' not found.\")\n",
    "else:\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform face detection\n",
    "    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "        results = face_detection.process(rgb_image)\n",
    "\n",
    "        if results.detections:\n",
    "            for i, detection in enumerate(results.detections):\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                h, w, _ = image.shape\n",
    "                margin = 0.2\n",
    "                x_min = int((bboxC.xmin - margin) * w)\n",
    "                y_min = int((bboxC.ymin - margin) * h)\n",
    "                x_max = int((bboxC.xmin + bboxC.width + margin) * w)\n",
    "                y_max = int((bboxC.ymin + bboxC.height + margin) * h)\n",
    "\n",
    "                x_min = max(0, x_min)\n",
    "                y_min = max(0, y_min)\n",
    "                x_max = min(w, x_max)\n",
    "                y_max = min(h, y_max)\n",
    "\n",
    "                cropped_face = image[y_min:y_max, x_min:x_max]\n",
    "                face_gray = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                output_path = os.path.join(output_dir, f\"testing_image_{i + 4}.jpg\")\n",
    "                cv2.imwrite(output_path, face_gray)\n",
    "                print(f\"Saved cropped face {i + 1} to '{output_path}'\")\n",
    "        else:\n",
    "            print(\"No faces detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353edd3-8f2a-4cfe-b823-2d6178be0e8d",
   "metadata": {},
   "source": [
    "# 2.Training Triplet loss model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40dfda7-b71f-43dd-9d41-ba4de715347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e861594f-8cfe-49b0-9783-cca460e30e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 12:51:43.397943: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-26 12:51:43.400400: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-26 12:51:43.432517: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-26 12:51:43.471764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732605703.510596   46790 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732605703.521280   46790 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-26 12:51:43.618361: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images: 147\n",
      "Unique labels: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 12:51:49.415534: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritika/anaconda3/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_8', 'keras_tensor_9', 'keras_tensor_10']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 313ms/step - loss: 0.3978\n",
      "Epoch 2/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 311ms/step - loss: 0.1918\n",
      "Epoch 3/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 325ms/step - loss: 0.1968\n",
      "Epoch 4/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 306ms/step - loss: 0.2250\n",
      "Epoch 5/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 337ms/step - loss: 0.2104\n",
      "Epoch 6/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 316ms/step - loss: 0.2017\n",
      "Epoch 7/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 312ms/step - loss: 0.1933\n",
      "Epoch 8/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 312ms/step - loss: 0.2007\n",
      "Epoch 9/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 308ms/step - loss: 0.2083\n",
      "Epoch 10/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 306ms/step - loss: 0.2047\n",
      "Triplet model saved to 'model_grayscale/triplet_model.keras'\n",
      "Embedding model saved to 'model_grayscale/embedding_model.keras'\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Validation Accuracy: 0.23\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         5\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.20      0.33         5\n",
      "           5       0.50      0.50      0.50         2\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.67      0.67      0.67         3\n",
      "           8       0.00      0.00      0.00         3\n",
      "           9       0.50      0.75      0.60         4\n",
      "\n",
      "    accuracy                           0.23        30\n",
      "   macro avg       0.27      0.21      0.21        30\n",
      "weighted avg       0.33      0.23      0.24        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritika/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ritika/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ritika/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ritika/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ritika/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ritika/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ritika/anaconda3/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_8', 'keras_tensor_9', 'keras_tensor_10']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss (Triplet): 0.20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Parameters\n",
    "IMAGE_SIZE = (112, 112)\n",
    "DATASET_PATH = \"new_grayscale/\"\n",
    "\n",
    "# Function to load and label images based on folder structure\n",
    "def load_images_from_directory(base_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    label_counter = 0\n",
    "\n",
    "    # Loop through each employee folder\n",
    "    for person_name in os.listdir(base_path):\n",
    "        person_path = os.path.join(base_path, person_name)\n",
    "        if os.path.isdir(person_path):\n",
    "            if person_name not in label_map:\n",
    "                label_map[person_name] = label_counter\n",
    "                label_counter += 1\n",
    "\n",
    "            # Load all images in the person's folder\n",
    "            for image_name in os.listdir(person_path):\n",
    "                image_path = os.path.join(person_path, image_name)\n",
    "                try:\n",
    "                    image = cv2.imread(image_path)\n",
    "                    if image is not None:\n",
    "                        image = cv2.resize(image, IMAGE_SIZE)\n",
    "                        data.append(image)\n",
    "                        labels.append(label_map[person_name])\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not read image {image_path}: {e}\")\n",
    "\n",
    "    return np.array(data), np.array(labels), label_map\n",
    "\n",
    "# Load dataset from the folder structure\n",
    "images, labels, label_map = load_images_from_directory(DATASET_PATH)\n",
    "\n",
    "# Normalize images (scaling pixel values to [0, 1])\n",
    "images = images / 255.0\n",
    "\n",
    "print(\"Loaded images:\", len(images))\n",
    "print(\"Unique labels:\", len(set(labels)))\n",
    "\n",
    "# Split dataset into training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model definition (Siamese-style network for triplet loss)\n",
    "def create_embedding_model(input_shape=IMAGE_SIZE + (3,)):\n",
    "    input = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(input)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    embedding = layers.Dense(64)(x)\n",
    "    model = Model(input, embedding)\n",
    "    return model\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    positive_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "    negative_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "    loss = tf.maximum(positive_dist - negative_dist + alpha, 0.0)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Register custom loss function\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "get_custom_objects()['triplet_loss'] = triplet_loss\n",
    "\n",
    "# Build the model\n",
    "embedding_model = create_embedding_model()\n",
    "\n",
    "# Triplet model\n",
    "anchor_input = layers.Input(shape=IMAGE_SIZE + (3,))\n",
    "positive_input = layers.Input(shape=IMAGE_SIZE + (3,))\n",
    "negative_input = layers.Input(shape=IMAGE_SIZE + (3,))\n",
    "\n",
    "# Get embeddings\n",
    "anchor_embedding = embedding_model(anchor_input)\n",
    "positive_embedding = embedding_model(positive_input)\n",
    "negative_embedding = embedding_model(negative_input)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "merged_output = layers.concatenate([anchor_embedding, positive_embedding, negative_embedding], axis=-1)\n",
    "\n",
    "# Define the model for triplet loss\n",
    "triplet_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=merged_output)\n",
    "\n",
    "# Compile the model with the triplet loss function\n",
    "triplet_model.compile(optimizer=Adam(learning_rate=0.0001), loss=triplet_loss)\n",
    "\n",
    "# Prepare triplets for training (anchor, positive, negative)\n",
    "def generate_triplets(X, y, num_triplets=1000):\n",
    "    anchors, positives, negatives = [], [], []\n",
    "    for _ in range(num_triplets):\n",
    "        # Pick a random class (person)\n",
    "        label = np.random.choice(np.unique(y))\n",
    "        # Get indices of the same class\n",
    "        same_class_indices = np.where(y == label)[0]\n",
    "        # Pick an anchor and positive image from the same class\n",
    "        anchor_idx, positive_idx = np.random.choice(same_class_indices, 2, replace=False)\n",
    "        # Pick a negative image from a different class\n",
    "        different_class_indices = np.where(y != label)[0]\n",
    "        negative_idx = np.random.choice(different_class_indices)\n",
    "\n",
    "        # Append to triplet arrays\n",
    "        anchors.append(X[anchor_idx])\n",
    "        positives.append(X[positive_idx])\n",
    "        negatives.append(X[negative_idx])\n",
    "\n",
    "    return np.array(anchors), np.array(positives), np.array(negatives)\n",
    "\n",
    "# Generate triplets for training\n",
    "anchors, positives, negatives = generate_triplets(X_train, y_train, num_triplets=5000)\n",
    "\n",
    "# Train the model on triplets\n",
    "triplet_model.fit(\n",
    "    [anchors, positives, negatives],\n",
    "    np.zeros(len(anchors)),  # Labels are not used for triplet loss\n",
    "    epochs=10,\n",
    "    batch_size=35\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "triplet_model.save('model_grayscale/triplet_model.keras')\n",
    "print(\"Triplet model saved to 'model_grayscale/triplet_model.keras'\")\n",
    "\n",
    "embedding_model.save('model_grayscale/embedding_model.keras')\n",
    "print(\"Embedding model saved to 'model_grayscale/embedding_model.keras'\")\n",
    "\n",
    "# Validation Code\n",
    "# Generate embeddings for the training and validation sets\n",
    "train_embeddings = embedding_model.predict(X_train)\n",
    "val_embeddings = embedding_model.predict(X_val)\n",
    "\n",
    "# Map training labels to their embeddings for easier lookup\n",
    "train_embeddings_map = {label: train_embeddings[np.where(y_train == label)] for label in np.unique(y_train)}\n",
    "\n",
    "# Predict validation labels using Nearest Neighbor classification\n",
    "y_val_pred = []\n",
    "for val_embedding in val_embeddings:\n",
    "    # Calculate distances from the validation embedding to all training embeddings\n",
    "    distances = {label: np.min(cdist([val_embedding], train_embeddings_map[label])) for label in train_embeddings_map}\n",
    "    # Predict the label with the smallest distance\n",
    "    predicted_label = min(distances, key=distances.get)\n",
    "    y_val_pred.append(predicted_label)\n",
    "\n",
    "# Compute validation accuracy\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification report (precision, recall, F1-score)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Generate triplets for validation\n",
    "val_anchors, val_positives, val_negatives = generate_triplets(X_val, y_val, num_triplets=500)\n",
    "\n",
    "# Evaluate triplet loss on validation triplets\n",
    "val_loss = triplet_model.evaluate([val_anchors, val_positives, val_negatives], np.zeros(len(val_anchors)), verbose=0)\n",
    "print(f\"Validation Loss (Triplet): {val_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1334d6-75f0-4850-a1df-72ae04909a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parameters\n",
    "IMAGE_SIZE = (112, 112)\n",
    "DATASET_PATH = \"new_grayscale/\"\n",
    "\n",
    "# Function to load and label images based on folder structure\n",
    "def load_images_from_directory(base_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    label_counter = 0\n",
    "\n",
    "    # Loop through each employee folder\n",
    "    for person_name in os.listdir(base_path):\n",
    "        person_path = os.path.join(base_path, person_name)\n",
    "        if os.path.isdir(person_path):\n",
    "            if person_name not in label_map:\n",
    "                label_map[person_name] = label_counter\n",
    "                label_counter += 1\n",
    "\n",
    "            # Load all images in the person's folder\n",
    "            for image_name in os.listdir(person_path):\n",
    "                image_path = os.path.join(person_path, image_name)\n",
    "                try:\n",
    "                    image = cv2.imread(image_path)\n",
    "                    if image is not None:\n",
    "                        image = cv2.resize(image, IMAGE_SIZE)\n",
    "                        data.append(image)\n",
    "                        labels.append(label_map[person_name])\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not read image {image_path}: {e}\")\n",
    "\n",
    "    return np.array(data), np.array(labels), label_map\n",
    "\n",
    "# Load dataset from the folder structure\n",
    "images, labels, label_map = load_images_from_directory(DATASET_PATH)\n",
    "\n",
    "# Normalize images (scaling pixel values to [0, 1])\n",
    "images = images / 255.0\n",
    "\n",
    "print(\"Loaded images:\", len(images))\n",
    "print(\"Unique labels:\", len(set(labels)))\n",
    "\n",
    "# Split dataset into training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model definition (Siamese-style network for triplet loss)\n",
    "def create_embedding_model(input_shape=IMAGE_SIZE + (3,)):\n",
    "    input = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(input)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    embedding = layers.Dense(64)(x)\n",
    "    model = Model(input, embedding)\n",
    "    return model\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    positive_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "    negative_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "    loss = tf.maximum(positive_dist - negative_dist + alpha, 0.0)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Register custom loss function\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "get_custom_objects()['triplet_loss'] = triplet_loss\n",
    "\n",
    "# Build the model\n",
    "embedding_model = create_embedding_model()\n",
    "\n",
    "# Triplet model\n",
    "anchor_input = layers.Input(shape=IMAGE_SIZE + (3,))\n",
    "positive_input = layers.Input(shape=IMAGE_SIZE + (3,))\n",
    "negative_input = layers.Input(shape=IMAGE_SIZE + (3,))\n",
    "\n",
    "# Get embeddings\n",
    "anchor_embedding = embedding_model(anchor_input)\n",
    "positive_embedding = embedding_model(positive_input)\n",
    "negative_embedding = embedding_model(negative_input)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "merged_output = layers.concatenate([anchor_embedding, positive_embedding, negative_embedding], axis=-1)\n",
    "\n",
    "# Define the model for triplet loss\n",
    "triplet_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=merged_output)\n",
    "\n",
    "# Compile the model with the triplet loss function\n",
    "triplet_model.compile(optimizer=Adam(learning_rate=0.0001), loss=triplet_loss)\n",
    "\n",
    "# Prepare triplets for training (anchor, positive, negative)\n",
    "def generate_triplets(X, y, num_triplets=1000):\n",
    "    anchors, positives, negatives = [], [], []\n",
    "    for _ in range(num_triplets):\n",
    "        # Pick a random class (person)\n",
    "        label = np.random.choice(np.unique(y))\n",
    "        # Get indices of the same class\n",
    "        same_class_indices = np.where(y == label)[0]\n",
    "        # Pick an anchor and positive image from the same class\n",
    "        anchor_idx, positive_idx = np.random.choice(same_class_indices, 2, replace=False)\n",
    "        # Pick a negative image from a different class\n",
    "        different_class_indices = np.where(y != label)[0]\n",
    "        negative_idx = np.random.choice(different_class_indices)\n",
    "\n",
    "        # Append to triplet arrays\n",
    "        anchors.append(X[anchor_idx])\n",
    "        positives.append(X[positive_idx])\n",
    "        negatives.append(X[negative_idx])\n",
    "\n",
    "    return np.array(anchors), np.array(positives), np.array(negatives)\n",
    "\n",
    "\n",
    "# Generate triplets for training\n",
    "anchors, positives, negatives = generate_triplets(X_train, y_train, num_triplets=5000)\n",
    "\n",
    "# Train the model on triplets\n",
    "triplet_model.fit(\n",
    "    [anchors, positives, negatives],\n",
    "    np.zeros(len(anchors)),  # Labels are not used for triplet loss\n",
    "    epochs=10,\n",
    "    batch_size=35\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "triplet_model.save('model_grayscale/triplet_model.keras')\n",
    "print(\"Triplet model saved to 'model_grayscale/triplet_model.keras'\")\n",
    "\n",
    "embedding_model.save('model_grayscale/embedding_model.keras')\n",
    "print(\"Embedding model saved to 'model_grayscale/embedding_model.keras'\")\n",
    "\n",
    "# Save embeddings for each employee during training\n",
    "def generate_embeddings_for_all_employees(X, y, label_map, embedding_model):\n",
    "    employee_embeddings = {}\n",
    "\n",
    "    # Iterate over each unique employee (identified by labels)\n",
    "    for label in np.unique(y):\n",
    "        # Get all images for this employee\n",
    "        employee_images = X[np.where(y == label)[0]]\n",
    "        employee_name = list(label_map.keys())[list(label_map.values()).index(label)]\n",
    "        \n",
    "        embeddings = []\n",
    "        \n",
    "        # Extract embeddings for each image of the employee\n",
    "        for img in employee_images:\n",
    "            img = np.expand_dims(img, axis=0)  # Add batch dimension for prediction\n",
    "            embedding = embedding_model.predict(img)  # Get the embedding for the image\n",
    "            embeddings.append(embedding.squeeze())  # Remove the batch dimension\n",
    "\n",
    "        # Average the embeddings of the employee (mean embedding)\n",
    "        employee_embeddings[employee_name] = np.mean(embeddings, axis=0)\n",
    "\n",
    "    # Save the embeddings to a file\n",
    "    embeddings_file_path = 'model_grayscale/employee_embeddings.npy'\n",
    "    np.save(embeddings_file_path, employee_embeddings)\n",
    "    print(f\"Saved employee embeddings to '{embeddings_file_path}'\")\n",
    "\n",
    "# Load dataset from the folder structure\n",
    "images, labels, label_map = load_images_from_directory(DATASET_PATH)\n",
    "images = images / 255.0  # Normalize images\n",
    "\n",
    "# Now, call the function to generate and save the embeddings\n",
    "generate_embeddings_for_all_employees(images, labels, label_map, embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c2bb9-8353-4e9e-a2b5-b041743f9309",
   "metadata": {},
   "source": [
    "# 3.Perform face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e14f7f-5a2b-42a6-abae-ff7ee40c5ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    positive_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "    negative_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "    loss = tf.maximum(positive_dist - negative_dist + alpha, 0.0)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "employee_embeddings = np.load('model_grayscale/employee_embeddings.npy', allow_pickle=True).item()\n",
    "\n",
    "embedding_model = tf.keras.models.load_model(\n",
    "    'model_grayscale/embedding_model.keras',\n",
    "    custom_objects={'triplet_loss': triplet_loss}\n",
    ")\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (112, 112))\n",
    "    image = image / 255.0\n",
    "    return np.expand_dims(image, axis=0)\n",
    "\n",
    "def get_embedding(image):\n",
    "    return embedding_model.predict(image).squeeze()\n",
    "\n",
    "def recognize_face(input_image_path, employee_embeddings, threshold=0.6):\n",
    "    input_image = preprocess_image(input_image_path)\n",
    "    input_embedding = get_embedding(input_image)\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    recognized_name = \"Unknown\"\n",
    "    for person_name, saved_embedding in employee_embeddings.items():\n",
    "        distance = cosine(input_embedding, saved_embedding)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            recognized_name = person_name\n",
    "\n",
    "    if min_distance < threshold:\n",
    "        return recognized_name\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "input_image_path = 'testing_data/testing_image_5.jpg'\n",
    "recognized_name = recognize_face(input_image_path, employee_embeddings)\n",
    "print(f\"Recognized: {recognized_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3666d6f-e600-40af-8795-b3dd645a01e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516522c-5150-406f-a01e-a3130fb8d5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
